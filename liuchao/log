[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: sequential
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.InjectPrefetch
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.InjectPrefetch
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        T.realize(A_red[0:4])
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.Buffer((32, 4))
        T.realize(A_red_rf[threadIdx_x:threadIdx_x + 1, threadIdx_y:threadIdx_y + 1])
        A_red_rf[threadIdx_x, threadIdx_y] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_red_rf[threadIdx_x, threadIdx_y] = A_red_rf[threadIdx_x, threadIdx_y] + A[(threadIdx_x + k0_k2_fused_outer * 32) // 3, threadIdx_y, (threadIdx_x + k0_k2_fused_outer * 32) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf[threadIdx_x, threadIdx_y], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.TextureFlatten
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.TextureFlatten
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.Buffer((32, 4))
        T.realize(A_red_rf[threadIdx_x:threadIdx_x + 1, threadIdx_y:threadIdx_y + 1])
        A_red_rf[threadIdx_x, threadIdx_y] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_red_rf[threadIdx_x, threadIdx_y] = A_red_rf[threadIdx_x, threadIdx_y] + A[(threadIdx_x + k0_k2_fused_outer * 32) // 3, threadIdx_y, (threadIdx_x + k0_k2_fused_outer * 32) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf[threadIdx_x, threadIdx_y], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.StorageFlatten
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.StorageFlatten_impl
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.BufferShapeLegalize
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.BufferShapeLegalize
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def dummy_func(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "global_symbol": "sum", "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.Buffer((1, 1))
        T.realize(A_red_rf[0:1, 0:1])
        A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y] = A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y] + A[(threadIdx_x + k0_k2_fused_outer * 32) // 3, threadIdx_y, (threadIdx_x + k0_k2_fused_outer * 32) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.BufferStrideLegalize
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.BufferStrideLegalize
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def dummy_func(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "global_symbol": "sum", "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.Buffer((1, 1))
        T.realize(A_red_rf[0:1, 0:1])
        A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y] = A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y] + A[(threadIdx_x + k0_k2_fused_outer * 32) // 3, threadIdx_y, (threadIdx_x + k0_k2_fused_outer * 32) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.ThreadScopePropagate
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.ThreadScopePropagate
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def dummy_func(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "global_symbol": "sum", "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.Buffer((1, 1), scope="local")
        T.realize(A_red_rf[0:1, 0:1])
        A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y] = A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y] + A[(threadIdx_x + k0_k2_fused_outer * 32) // 3, threadIdx_y, (threadIdx_x + k0_k2_fused_outer * 32) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.BufferBindUnwrapper
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.BufferBindUnwrapper
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def dummy_func(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "global_symbol": "sum", "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.Buffer((1, 1), scope="local")
        T.realize(A_red_rf[0:1, 0:1])
        A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y] = A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y] + A[(threadIdx_x + k0_k2_fused_outer * 32) // 3, threadIdx_y, (threadIdx_x + k0_k2_fused_outer * 32) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.ApplyLayoutTransforms
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.ApplyLayoutTransforms
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def dummy_func(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "global_symbol": "sum", "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.Buffer((1, 1), scope="local")
        T.realize(A_red_rf[0:1, 0:1])
        A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y] = A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y] + A[(threadIdx_x + k0_k2_fused_outer * 32) // 3, threadIdx_y, (threadIdx_x + k0_k2_fused_outer * 32) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf[threadIdx_x - threadIdx_x, threadIdx_y - threadIdx_y], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.StorageFlattener
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.StorageFlattener
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def dummy_func(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "global_symbol": "sum", "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.AssertSimplifier
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.AssertSimplifier
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def dummy_func(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "global_symbol": "sum", "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.StorageFlatten_impl
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def dummy_func(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "global_symbol": "sum", "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.StorageFlatten
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LowerCrossThreadReduction
[16:36:37] /ssd2/liuchao52/tvm/src/tir/transforms/lower_cross_thread_reduction.cc:902: Warning: 6clc# from tvm.script import tir as T

@T.prim_func
def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
    T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", 1)
    threadIdx_y = T.launch_thread("threadIdx.y", 32)
    threadIdx_x = T.launch_thread("threadIdx.x", 32)
    A_red_rf = T.allocate([1], "float32", "local")
    A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
    A_red_rf_1[0] = T.float32(0)
    for k0_k2_fused_outer in range(2):
        if T.likely(threadIdx_y < 4):
            if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                A_1 = T.Buffer((192,), data=A.data)
                A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
    reduce_temp0 = T.allocate([1], "float32", "local")
    reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
    with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
        T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
    if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
        if threadIdx_x == 0:
            if T.bool(True):
                A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/tir/transforms/lower_cross_thread_reduction.cc:904: Warning: # from tvm.script import tir as T

@T.prim_func
def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
    T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", 1)
    threadIdx_y = T.launch_thread("threadIdx.y", 32)
    threadIdx_x = T.launch_thread("threadIdx.x", 32)
    A_red_rf = T.allocate([1], "float32", "local")
    A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
    A_red_rf_1[0] = T.float32(0)
    for k0_k2_fused_outer in range(2):
        if T.likely(threadIdx_y < 4):
            if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                A_1 = T.Buffer((192,), data=A.data)
                A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
    reduce_temp0 = T.allocate([1], "float32", "local")
    reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
    with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
        T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
    if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
        if threadIdx_x == 0:
            if T.bool(True):
                A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LowerCrossThreadReduction
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LowerInitBlock
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LowerInitBlock
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.PlanAndUpdateBufferAllocationLocation
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.PlanAndUpdateBufferAllocationLocation
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.ConvertBlocksToOpaque
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.ConvertBlocksToOpaque
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LiftThreadBinding
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LiftThreadBinding
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.ManifestSharedMemoryLocalStage
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.ManifestSharedMemoryLocalStage
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.CompactBufferAllocation
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.CompactBufferAllocation
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LowerAutoCopy
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LowerAutoCopy
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.UnifyThreadBinding
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.UnifyThreadBinding
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LowerMatchBuffer
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LowerMatchBuffer
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely((threadIdx_x + k0_k2_fused_outer * 32) // 3 < 16 and threadIdx_x + k0_k2_fused_outer * 32 < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and blockIdx_x * 32 + threadIdx_y < 4:
            if threadIdx_x == 0:
                if T.bool(True):
                    A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.Simplify
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.Simplify
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely(k0_k2_fused_outer * 32 + threadIdx_x < 48 and k0_k2_fused_outer * 32 + threadIdx_x < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.InjectPermutedLayout
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.InjectPermutedLayout
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely(k0_k2_fused_outer * 32 + threadIdx_x < 48 and k0_k2_fused_outer * 32 + threadIdx_x < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.Simplify
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.Simplify
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely(k0_k2_fused_outer * 32 + threadIdx_x < 48 and k0_k2_fused_outer * 32 + threadIdx_x < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.InjectSoftwarePipeline
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.InjectSoftwarePipeline
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely(k0_k2_fused_outer * 32 + threadIdx_x < 48 and k0_k2_fused_outer * 32 + threadIdx_x < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.TransformMmaBufferLayout
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.TransformMmaBufferLayout
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely(k0_k2_fused_outer * 32 + threadIdx_x < 48 and k0_k2_fused_outer * 32 + threadIdx_x < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LowerOpaqueBlock
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LowerOpaqueBlock
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely(k0_k2_fused_outer * 32 + threadIdx_x < 48 and k0_k2_fused_outer * 32 + threadIdx_x < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.FlattenBuffer
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.FlattenBuffer
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely(k0_k2_fused_outer * 32 + threadIdx_x < 48 and k0_k2_fused_outer * 32 + threadIdx_x < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.FP8ComputeLegalize
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.FP8ComputeLegalize
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely(k0_k2_fused_outer * 32 + threadIdx_x < 48 and k0_k2_fused_outer * 32 + threadIdx_x < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.BF16ComputeLegalize
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.BF16ComputeLegalize
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely(k0_k2_fused_outer * 32 + threadIdx_x < 48 and k0_k2_fused_outer * 32 + threadIdx_x < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.NarrowDataType
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.NarrowDataType
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely(k0_k2_fused_outer * 32 + threadIdx_x < 48 and k0_k2_fused_outer * 32 + threadIdx_x < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.Simplify
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.Simplify
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if T.likely(threadIdx_y < 4):
                if T.likely(k0_k2_fused_outer * 32 + threadIdx_x < 48 and k0_k2_fused_outer * 32 + threadIdx_x < 48):
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LoopPartition
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LoopPartition
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 32 + threadIdx_x < 48 and k0_k2_fused_outer * 32 + threadIdx_x < 48:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.VectorizeLoop
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.VectorizeLoop
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 32 + threadIdx_x < 48 and k0_k2_fused_outer * 32 + threadIdx_x < 48:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.InjectVirtualThread
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.InjectVirtualThread
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 32 + threadIdx_x < 48 and k0_k2_fused_outer * 32 + threadIdx_x < 48:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.InjectDoubleBuffer
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.InjectDoubleBuffer
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf = T.allocate([1], "float32", "local")
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 32 + threadIdx_x < 48 and k0_k2_fused_outer * 32 + threadIdx_x < 48:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0 = T.allocate([1], "float32", "local")
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.StorageRewrite
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.StorageRewrite
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 32 + threadIdx_x < 48 and k0_k2_fused_outer * 32 + threadIdx_x < 48:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.UnrollLoop
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.UnrollLoop
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 32 + threadIdx_x < 48 and k0_k2_fused_outer * 32 + threadIdx_x < 48:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.RenormalizeSplitPattern
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.RenormalizeSplitPattern
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y // 4 < 1:
                if (k0_k2_fused_outer * 2 + threadIdx_x // 16) // 3 < 1 and (k0_k2_fused_outer * 2 + threadIdx_x // 16) // 3 < 1:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y // 4 < 1:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.Simplify
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.Simplify
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.RemoveNoOp
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.RemoveNoOp
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.RewriteUnsafeSelect
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.RewriteUnsafeSelect
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.HoistIfThenElse
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.InsertHoistIfThenElse
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.InsertHoistIfThenElse
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.Simplify
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.Simplify
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.RemoveNoOp
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.RemoveNoOp
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.HoistIfThenElse
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.CommonSubexprElimTIR
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.CommonSubexprElimTIR
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: sequential
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/driver/driver_api.cc:416: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: sequential
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.calculate_allocated_bytes
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.calculate_allocated_bytes
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LowerVtcmAlloc
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LowerVtcmAlloc
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.BindTarget
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.BindTarget
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.VerifyMemory
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.VerifyMemory
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.AnnotateEntryFunc
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.AnnotateEntryFunc
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.ThreadSync
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.ThreadSync
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.ThreadSync
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.ThreadSync
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.MergeDynamicSharedMemoryAllocations
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.MergeDynamicSharedMemoryAllocations
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.ThreadSync
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.ThreadSync
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.InferFragment
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.InferFragment
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LowerThreadAllreduce
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LowerThreadAllreduce
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tvm_warp_activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 16, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 8, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 4, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 2, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 1, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tvm_warp_shuffle(mask_1[0], red_buf0_1[0], 32 * threadIdx_y, 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = red_buf0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.AnnotateDeviceRegions
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.AnnotateDeviceRegions
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        T.attr(T.target({"arch": "sm_80", "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "target", 0)
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tvm_warp_activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 16, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 8, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 4, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 2, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 1, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tvm_warp_shuffle(mask_1[0], red_buf0_1[0], 32 * threadIdx_y, 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = red_buf0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.SplitHostDevice
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.ConvertSSA
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.ConvertSSA
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        Module.sum_kernel(A.data, A_red.data)

    @T.prim_func(private=True)
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"target": T.target({"arch": "sm_80", "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tvm_warp_activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 16, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 8, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 4, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 2, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 1, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tvm_warp_shuffle(mask_1[0], red_buf0_1[0], 32 * threadIdx_y, 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.SplitHostDevice
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        Module.sum_kernel(A.data, A_red.data)

    @T.prim_func(private=True)
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"target": T.target({"arch": "sm_80", "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tvm_warp_activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 16, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 8, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 4, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 2, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 1, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tvm_warp_shuffle(mask_1[0], red_buf0_1[0], 32 * threadIdx_y, 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.MakePackedAPI
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.MakePackedAPI
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(args: T.handle, arg_type_ids: T.handle("int32"), num_args: T.int32, out_ret_value: T.handle("void"), out_ret_tcode: T.handle("int32"), resource_handle: T.handle) -> T.int32:
        T.func_attr({"calling_conv": 1, "from_legacy_te_schedule": T.bool(True), "target": T.target({"keys": ["cpu"], "kind": "llvm", "tag": ""}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        assert num_args == 2, "sum: num_args should be 2"
        arg_type_ids_1 = T.decl_buffer((2,), "int32", data=arg_type_ids)
        A_code: T.int32 = arg_type_ids_1[0]
        A_red_code: T.int32 = arg_type_ids_1[1]
        A: T.handle = T.tvm_struct_get(args, 0, 12, "handle")
        A_red: T.handle = T.tvm_struct_get(args, 1, 12, "handle")
        A_1: T.handle("float32") = T.tvm_struct_get(A, 0, 1, "handle")
        T.attr(A_1, "storage_alignment", 64)
        sum_A_shape: T.handle("int64") = T.tvm_struct_get(A, 0, 2, "handle")
        sum_A_shape_1 = T.decl_buffer((3,), "int64", data=sum_A_shape)
        sum_A_strides: T.handle("int64") = T.tvm_struct_get(A, 0, 3, "handle")
        sum_A_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_strides)
        dev_id: T.int32 = T.tvm_struct_get(A, 0, 9, "int32")
        A_red_1: T.handle("float32") = T.tvm_struct_get(A_red, 0, 1, "handle")
        T.attr(A_red_1, "storage_alignment", 64)
        sum_A_red_shape: T.handle("int64") = T.tvm_struct_get(A_red, 0, 2, "handle")
        sum_A_red_shape_1 = T.decl_buffer((1,), "int64", data=sum_A_red_shape)
        sum_A_red_strides: T.handle("int64") = T.tvm_struct_get(A_red, 0, 3, "handle")
        sum_A_red_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_red_strides)
        assert A_code == 3 or A_code == 13 or A_code == 7 or A_code == 4, "sum: Expect arg[0] to be pointer"
        assert A_red_code == 3 or A_red_code == 13 or A_red_code == 7 or A_red_code == 4, "sum: Expect arg[1] to be pointer"
        T.attr("default", "device_id", dev_id)
        T.attr("default", "device_type", 2)
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert T.tvm_struct_get(A, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A, 0, 7, "uint16") == T.uint16(1), "sum.A.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_shape_1[0]) == 16, "Argument sum.A.shape[0] has an unsatisfied constraint: 16 == T.Cast(\"int32\", sum_A_shape[0])"
        assert T.Cast("int32", sum_A_shape_1[1]) == 4, "Argument sum.A.shape[1] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_shape[1])"
        assert T.Cast("int32", sum_A_shape_1[2]) == 3, "Argument sum.A.shape[2] has an unsatisfied constraint: 3 == T.Cast(\"int32\", sum_A_shape[2])"
        if not T.isnullptr(sum_A_strides):
            assert 1 == T.Cast("int32", sum_A_strides_1[2]) and 3 == T.Cast("int32", sum_A_strides_1[1]) and 12 == T.Cast("int32", sum_A_strides_1[0]), "sum.A.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A, 0, 8, "uint64"), "Argument sum.A.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A, 0, 10, "int32") == 2, "Argument sum.A.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A, 0, 10, \"int32\")"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert T.tvm_struct_get(A_red, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A_red, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A_red, 0, 7, "uint16") == T.uint16(1), "sum.A_red.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_red_shape_1[0]) == 4, "Argument sum.A_red.shape[0] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_red_shape[0])"
        if not T.isnullptr(sum_A_red_strides):
            assert 1 == T.Cast("int32", sum_A_red_strides_1[0]), "sum.A_red.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, "uint64"), "Argument sum.A_red.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A_red, 0, 10, "int32") == 2, "Argument sum.A_red.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A_red, 0, 10, \"int32\")"
        assert dev_id == T.tvm_struct_get(A_red, 0, 9, "int32"), "Argument sum.A_red.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(A_red, 0, 9, \"int32\")"
        A_2 = T.decl_buffer((16, 4, 3), data=A_1)
        A_red_2 = T.decl_buffer((4,), data=A_red_1)
        T.call_packed("__tvm_set_device", 2, dev_id)
        with T.attr(0, "compute_scope", "sum_compute_"):
            Module.sum_kernel(A_1, A_red_1)
        T.ret(0)

    @T.prim_func(private=True)
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"target": T.target({"arch": "sm_80", "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tvm_warp_activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 16, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 8, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 4, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 2, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 1, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tvm_warp_shuffle(mask_1[0], red_buf0_1[0], 32 * threadIdx_y, 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.FP8StorageLegalize
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.FP8StorageLegalize
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(args: T.handle, arg_type_ids: T.handle("int32"), num_args: T.int32, out_ret_value: T.handle("void"), out_ret_tcode: T.handle("int32"), resource_handle: T.handle) -> T.int32:
        T.func_attr({"calling_conv": 1, "from_legacy_te_schedule": T.bool(True), "target": T.target({"keys": ["cpu"], "kind": "llvm", "tag": ""}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        assert num_args == 2, "sum: num_args should be 2"
        arg_type_ids_1 = T.decl_buffer((2,), "int32", data=arg_type_ids)
        A_code: T.int32 = arg_type_ids_1[0]
        A_red_code: T.int32 = arg_type_ids_1[1]
        A: T.handle = T.tvm_struct_get(args, 0, 12, "handle")
        A_red: T.handle = T.tvm_struct_get(args, 1, 12, "handle")
        A_1: T.handle("float32") = T.tvm_struct_get(A, 0, 1, "handle")
        T.attr(A_1, "storage_alignment", 64)
        sum_A_shape: T.handle("int64") = T.tvm_struct_get(A, 0, 2, "handle")
        sum_A_shape_1 = T.decl_buffer((3,), "int64", data=sum_A_shape)
        sum_A_strides: T.handle("int64") = T.tvm_struct_get(A, 0, 3, "handle")
        sum_A_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_strides)
        dev_id: T.int32 = T.tvm_struct_get(A, 0, 9, "int32")
        A_red_1: T.handle("float32") = T.tvm_struct_get(A_red, 0, 1, "handle")
        T.attr(A_red_1, "storage_alignment", 64)
        sum_A_red_shape: T.handle("int64") = T.tvm_struct_get(A_red, 0, 2, "handle")
        sum_A_red_shape_1 = T.decl_buffer((1,), "int64", data=sum_A_red_shape)
        sum_A_red_strides: T.handle("int64") = T.tvm_struct_get(A_red, 0, 3, "handle")
        sum_A_red_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_red_strides)
        assert A_code == 3 or A_code == 13 or A_code == 7 or A_code == 4, "sum: Expect arg[0] to be pointer"
        assert A_red_code == 3 or A_red_code == 13 or A_red_code == 7 or A_red_code == 4, "sum: Expect arg[1] to be pointer"
        T.attr("default", "device_id", dev_id)
        T.attr("default", "device_type", 2)
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert T.tvm_struct_get(A, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A, 0, 7, "uint16") == T.uint16(1), "sum.A.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_shape_1[0]) == 16, "Argument sum.A.shape[0] has an unsatisfied constraint: 16 == T.Cast(\"int32\", sum_A_shape[0])"
        assert T.Cast("int32", sum_A_shape_1[1]) == 4, "Argument sum.A.shape[1] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_shape[1])"
        assert T.Cast("int32", sum_A_shape_1[2]) == 3, "Argument sum.A.shape[2] has an unsatisfied constraint: 3 == T.Cast(\"int32\", sum_A_shape[2])"
        if not T.isnullptr(sum_A_strides):
            assert 1 == T.Cast("int32", sum_A_strides_1[2]) and 3 == T.Cast("int32", sum_A_strides_1[1]) and 12 == T.Cast("int32", sum_A_strides_1[0]), "sum.A.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A, 0, 8, "uint64"), "Argument sum.A.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A, 0, 10, "int32") == 2, "Argument sum.A.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A, 0, 10, \"int32\")"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert T.tvm_struct_get(A_red, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A_red, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A_red, 0, 7, "uint16") == T.uint16(1), "sum.A_red.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_red_shape_1[0]) == 4, "Argument sum.A_red.shape[0] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_red_shape[0])"
        if not T.isnullptr(sum_A_red_strides):
            assert 1 == T.Cast("int32", sum_A_red_strides_1[0]), "sum.A_red.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, "uint64"), "Argument sum.A_red.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A_red, 0, 10, "int32") == 2, "Argument sum.A_red.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A_red, 0, 10, \"int32\")"
        assert dev_id == T.tvm_struct_get(A_red, 0, 9, "int32"), "Argument sum.A_red.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(A_red, 0, 9, \"int32\")"
        A_2 = T.decl_buffer((16, 4, 3), data=A_1)
        A_red_2 = T.decl_buffer((4,), data=A_red_1)
        T.call_packed("__tvm_set_device", 2, dev_id)
        with T.attr(0, "compute_scope", "sum_compute_"):
            Module.sum_kernel(A_1, A_red_1)
        T.ret(0)

    @T.prim_func(private=True)
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"target": T.target({"arch": "sm_80", "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tvm_warp_activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 16, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 8, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 4, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 2, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 1, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tvm_warp_shuffle(mask_1[0], red_buf0_1[0], 32 * threadIdx_y, 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.BF16StorageLegalize
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.BF16StorageLegalize
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(args: T.handle, arg_type_ids: T.handle("int32"), num_args: T.int32, out_ret_value: T.handle("void"), out_ret_tcode: T.handle("int32"), resource_handle: T.handle) -> T.int32:
        T.func_attr({"calling_conv": 1, "from_legacy_te_schedule": T.bool(True), "target": T.target({"keys": ["cpu"], "kind": "llvm", "tag": ""}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        assert num_args == 2, "sum: num_args should be 2"
        arg_type_ids_1 = T.decl_buffer((2,), "int32", data=arg_type_ids)
        A_code: T.int32 = arg_type_ids_1[0]
        A_red_code: T.int32 = arg_type_ids_1[1]
        A: T.handle = T.tvm_struct_get(args, 0, 12, "handle")
        A_red: T.handle = T.tvm_struct_get(args, 1, 12, "handle")
        A_1: T.handle("float32") = T.tvm_struct_get(A, 0, 1, "handle")
        T.attr(A_1, "storage_alignment", 64)
        sum_A_shape: T.handle("int64") = T.tvm_struct_get(A, 0, 2, "handle")
        sum_A_shape_1 = T.decl_buffer((3,), "int64", data=sum_A_shape)
        sum_A_strides: T.handle("int64") = T.tvm_struct_get(A, 0, 3, "handle")
        sum_A_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_strides)
        dev_id: T.int32 = T.tvm_struct_get(A, 0, 9, "int32")
        A_red_1: T.handle("float32") = T.tvm_struct_get(A_red, 0, 1, "handle")
        T.attr(A_red_1, "storage_alignment", 64)
        sum_A_red_shape: T.handle("int64") = T.tvm_struct_get(A_red, 0, 2, "handle")
        sum_A_red_shape_1 = T.decl_buffer((1,), "int64", data=sum_A_red_shape)
        sum_A_red_strides: T.handle("int64") = T.tvm_struct_get(A_red, 0, 3, "handle")
        sum_A_red_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_red_strides)
        assert A_code == 3 or A_code == 13 or A_code == 7 or A_code == 4, "sum: Expect arg[0] to be pointer"
        assert A_red_code == 3 or A_red_code == 13 or A_red_code == 7 or A_red_code == 4, "sum: Expect arg[1] to be pointer"
        T.attr("default", "device_id", dev_id)
        T.attr("default", "device_type", 2)
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert T.tvm_struct_get(A, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A, 0, 7, "uint16") == T.uint16(1), "sum.A.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_shape_1[0]) == 16, "Argument sum.A.shape[0] has an unsatisfied constraint: 16 == T.Cast(\"int32\", sum_A_shape[0])"
        assert T.Cast("int32", sum_A_shape_1[1]) == 4, "Argument sum.A.shape[1] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_shape[1])"
        assert T.Cast("int32", sum_A_shape_1[2]) == 3, "Argument sum.A.shape[2] has an unsatisfied constraint: 3 == T.Cast(\"int32\", sum_A_shape[2])"
        if not T.isnullptr(sum_A_strides):
            assert 1 == T.Cast("int32", sum_A_strides_1[2]) and 3 == T.Cast("int32", sum_A_strides_1[1]) and 12 == T.Cast("int32", sum_A_strides_1[0]), "sum.A.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A, 0, 8, "uint64"), "Argument sum.A.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A, 0, 10, "int32") == 2, "Argument sum.A.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A, 0, 10, \"int32\")"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert T.tvm_struct_get(A_red, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A_red, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A_red, 0, 7, "uint16") == T.uint16(1), "sum.A_red.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_red_shape_1[0]) == 4, "Argument sum.A_red.shape[0] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_red_shape[0])"
        if not T.isnullptr(sum_A_red_strides):
            assert 1 == T.Cast("int32", sum_A_red_strides_1[0]), "sum.A_red.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, "uint64"), "Argument sum.A_red.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A_red, 0, 10, "int32") == 2, "Argument sum.A_red.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A_red, 0, 10, \"int32\")"
        assert dev_id == T.tvm_struct_get(A_red, 0, 9, "int32"), "Argument sum.A_red.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(A_red, 0, 9, \"int32\")"
        A_2 = T.decl_buffer((16, 4, 3), data=A_1)
        A_red_2 = T.decl_buffer((4,), data=A_red_1)
        T.call_packed("__tvm_set_device", 2, dev_id)
        with T.attr(0, "compute_scope", "sum_compute_"):
            Module.sum_kernel(A_1, A_red_1)
        T.ret(0)

    @T.prim_func(private=True)
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"target": T.target({"arch": "sm_80", "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tvm_warp_activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 16, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 8, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 4, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 2, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 1, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tvm_warp_shuffle(mask_1[0], red_buf0_1[0], 32 * threadIdx_y, 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LowerDeviceKernelLaunch
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LowerDeviceKernelLaunch
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(args: T.handle, arg_type_ids: T.handle("int32"), num_args: T.int32, out_ret_value: T.handle("void"), out_ret_tcode: T.handle("int32"), resource_handle: T.handle) -> T.int32:
        T.func_attr({"calling_conv": 1, "from_legacy_te_schedule": T.bool(True), "target": T.target({"keys": ["cpu"], "kind": "llvm", "tag": ""}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        assert num_args == 2, "sum: num_args should be 2"
        arg_type_ids_1 = T.decl_buffer((2,), "int32", data=arg_type_ids)
        A_code: T.int32 = arg_type_ids_1[0]
        A_red_code: T.int32 = arg_type_ids_1[1]
        A: T.handle = T.tvm_struct_get(args, 0, 12, "handle")
        A_red: T.handle = T.tvm_struct_get(args, 1, 12, "handle")
        A_1: T.handle("float32") = T.tvm_struct_get(A, 0, 1, "handle")
        T.attr(A_1, "storage_alignment", 64)
        sum_A_shape: T.handle("int64") = T.tvm_struct_get(A, 0, 2, "handle")
        sum_A_shape_1 = T.decl_buffer((3,), "int64", data=sum_A_shape)
        sum_A_strides: T.handle("int64") = T.tvm_struct_get(A, 0, 3, "handle")
        sum_A_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_strides)
        dev_id: T.int32 = T.tvm_struct_get(A, 0, 9, "int32")
        A_red_1: T.handle("float32") = T.tvm_struct_get(A_red, 0, 1, "handle")
        T.attr(A_red_1, "storage_alignment", 64)
        sum_A_red_shape: T.handle("int64") = T.tvm_struct_get(A_red, 0, 2, "handle")
        sum_A_red_shape_1 = T.decl_buffer((1,), "int64", data=sum_A_red_shape)
        sum_A_red_strides: T.handle("int64") = T.tvm_struct_get(A_red, 0, 3, "handle")
        sum_A_red_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_red_strides)
        assert A_code == 3 or A_code == 13 or A_code == 7 or A_code == 4, "sum: Expect arg[0] to be pointer"
        assert A_red_code == 3 or A_red_code == 13 or A_red_code == 7 or A_red_code == 4, "sum: Expect arg[1] to be pointer"
        T.attr("default", "device_id", dev_id)
        T.attr("default", "device_type", 2)
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert T.tvm_struct_get(A, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A, 0, 7, "uint16") == T.uint16(1), "sum.A.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_shape_1[0]) == 16, "Argument sum.A.shape[0] has an unsatisfied constraint: 16 == T.Cast(\"int32\", sum_A_shape[0])"
        assert T.Cast("int32", sum_A_shape_1[1]) == 4, "Argument sum.A.shape[1] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_shape[1])"
        assert T.Cast("int32", sum_A_shape_1[2]) == 3, "Argument sum.A.shape[2] has an unsatisfied constraint: 3 == T.Cast(\"int32\", sum_A_shape[2])"
        if not T.isnullptr(sum_A_strides):
            assert 1 == T.Cast("int32", sum_A_strides_1[2]) and 3 == T.Cast("int32", sum_A_strides_1[1]) and 12 == T.Cast("int32", sum_A_strides_1[0]), "sum.A.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A, 0, 8, "uint64"), "Argument sum.A.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A, 0, 10, "int32") == 2, "Argument sum.A.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A, 0, 10, \"int32\")"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert T.tvm_struct_get(A_red, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A_red, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A_red, 0, 7, "uint16") == T.uint16(1), "sum.A_red.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_red_shape_1[0]) == 4, "Argument sum.A_red.shape[0] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_red_shape[0])"
        if not T.isnullptr(sum_A_red_strides):
            assert 1 == T.Cast("int32", sum_A_red_strides_1[0]), "sum.A_red.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, "uint64"), "Argument sum.A_red.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A_red, 0, 10, "int32") == 2, "Argument sum.A_red.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A_red, 0, 10, \"int32\")"
        assert dev_id == T.tvm_struct_get(A_red, 0, 9, "int32"), "Argument sum.A_red.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(A_red, 0, 9, \"int32\")"
        A_2 = T.decl_buffer((16, 4, 3), data=A_1)
        A_red_2 = T.decl_buffer((4,), data=A_red_1)
        T.call_packed("__tvm_set_device", 2, dev_id)
        with T.attr(0, "compute_scope", "sum_compute_"):
            T.call_packed("sum_kernel", A_1, A_red_1, 1, 32, 32)
        T.ret(0)

    @T.prim_func
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"calling_conv": 2, "target": T.target({"arch": "sm_80", "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.kernel_launch_params": ["blockIdx.x", "threadIdx.y", "threadIdx.x"], "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tvm_warp_activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 16, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 8, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 4, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 2, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 1, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tvm_warp_shuffle(mask_1[0], red_buf0_1[0], 32 * threadIdx_y, 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: sequential
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(args: T.handle, arg_type_ids: T.handle("int32"), num_args: T.int32, out_ret_value: T.handle("void"), out_ret_tcode: T.handle("int32"), resource_handle: T.handle) -> T.int32:
        T.func_attr({"calling_conv": 1, "from_legacy_te_schedule": T.bool(True), "target": T.target({"keys": ["cpu"], "kind": "llvm", "tag": ""}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        assert num_args == 2, "sum: num_args should be 2"
        arg_type_ids_1 = T.decl_buffer((2,), "int32", data=arg_type_ids)
        A_code: T.int32 = arg_type_ids_1[0]
        A_red_code: T.int32 = arg_type_ids_1[1]
        A: T.handle = T.tvm_struct_get(args, 0, 12, "handle")
        A_red: T.handle = T.tvm_struct_get(args, 1, 12, "handle")
        A_1: T.handle("float32") = T.tvm_struct_get(A, 0, 1, "handle")
        T.attr(A_1, "storage_alignment", 64)
        sum_A_shape: T.handle("int64") = T.tvm_struct_get(A, 0, 2, "handle")
        sum_A_shape_1 = T.decl_buffer((3,), "int64", data=sum_A_shape)
        sum_A_strides: T.handle("int64") = T.tvm_struct_get(A, 0, 3, "handle")
        sum_A_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_strides)
        dev_id: T.int32 = T.tvm_struct_get(A, 0, 9, "int32")
        A_red_1: T.handle("float32") = T.tvm_struct_get(A_red, 0, 1, "handle")
        T.attr(A_red_1, "storage_alignment", 64)
        sum_A_red_shape: T.handle("int64") = T.tvm_struct_get(A_red, 0, 2, "handle")
        sum_A_red_shape_1 = T.decl_buffer((1,), "int64", data=sum_A_red_shape)
        sum_A_red_strides: T.handle("int64") = T.tvm_struct_get(A_red, 0, 3, "handle")
        sum_A_red_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_red_strides)
        assert A_code == 3 or A_code == 13 or A_code == 7 or A_code == 4, "sum: Expect arg[0] to be pointer"
        assert A_red_code == 3 or A_red_code == 13 or A_red_code == 7 or A_red_code == 4, "sum: Expect arg[1] to be pointer"
        T.attr("default", "device_id", dev_id)
        T.attr("default", "device_type", 2)
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert T.tvm_struct_get(A, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A, 0, 7, "uint16") == T.uint16(1), "sum.A.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_shape_1[0]) == 16, "Argument sum.A.shape[0] has an unsatisfied constraint: 16 == T.Cast(\"int32\", sum_A_shape[0])"
        assert T.Cast("int32", sum_A_shape_1[1]) == 4, "Argument sum.A.shape[1] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_shape[1])"
        assert T.Cast("int32", sum_A_shape_1[2]) == 3, "Argument sum.A.shape[2] has an unsatisfied constraint: 3 == T.Cast(\"int32\", sum_A_shape[2])"
        if not T.isnullptr(sum_A_strides):
            assert 1 == T.Cast("int32", sum_A_strides_1[2]) and 3 == T.Cast("int32", sum_A_strides_1[1]) and 12 == T.Cast("int32", sum_A_strides_1[0]), "sum.A.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A, 0, 8, "uint64"), "Argument sum.A.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A, 0, 10, "int32") == 2, "Argument sum.A.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A, 0, 10, \"int32\")"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert T.tvm_struct_get(A_red, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A_red, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A_red, 0, 7, "uint16") == T.uint16(1), "sum.A_red.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_red_shape_1[0]) == 4, "Argument sum.A_red.shape[0] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_red_shape[0])"
        if not T.isnullptr(sum_A_red_strides):
            assert 1 == T.Cast("int32", sum_A_red_strides_1[0]), "sum.A_red.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, "uint64"), "Argument sum.A_red.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A_red, 0, 10, "int32") == 2, "Argument sum.A_red.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A_red, 0, 10, \"int32\")"
        assert dev_id == T.tvm_struct_get(A_red, 0, 9, "int32"), "Argument sum.A_red.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(A_red, 0, 9, \"int32\")"
        A_2 = T.decl_buffer((16, 4, 3), data=A_1)
        A_red_2 = T.decl_buffer((4,), data=A_red_1)
        T.call_packed("__tvm_set_device", 2, dev_id)
        with T.attr(0, "compute_scope", "sum_compute_"):
            T.call_packed("sum_kernel", A_1, A_red_1, 1, 32, 32)
        T.ret(0)

    @T.prim_func
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"calling_conv": 2, "target": T.target({"arch": "sm_80", "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.kernel_launch_params": ["blockIdx.x", "threadIdx.y", "threadIdx.x"], "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tvm_warp_activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 16, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 8, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 4, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 2, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 1, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tvm_warp_shuffle(mask_1[0], red_buf0_1[0], 32 * threadIdx_y, 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/driver/driver_api.cc:418: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(args: T.handle, arg_type_ids: T.handle("int32"), num_args: T.int32, out_ret_value: T.handle("void"), out_ret_tcode: T.handle("int32"), resource_handle: T.handle) -> T.int32:
        T.func_attr({"calling_conv": 1, "from_legacy_te_schedule": T.bool(True), "target": T.target({"keys": ["cpu"], "kind": "llvm", "tag": ""}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        assert num_args == 2, "sum: num_args should be 2"
        arg_type_ids_1 = T.decl_buffer((2,), "int32", data=arg_type_ids)
        A_code: T.int32 = arg_type_ids_1[0]
        A_red_code: T.int32 = arg_type_ids_1[1]
        A: T.handle = T.tvm_struct_get(args, 0, 12, "handle")
        A_red: T.handle = T.tvm_struct_get(args, 1, 12, "handle")
        A_1: T.handle("float32") = T.tvm_struct_get(A, 0, 1, "handle")
        T.attr(A_1, "storage_alignment", 64)
        sum_A_shape: T.handle("int64") = T.tvm_struct_get(A, 0, 2, "handle")
        sum_A_shape_1 = T.decl_buffer((3,), "int64", data=sum_A_shape)
        sum_A_strides: T.handle("int64") = T.tvm_struct_get(A, 0, 3, "handle")
        sum_A_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_strides)
        dev_id: T.int32 = T.tvm_struct_get(A, 0, 9, "int32")
        A_red_1: T.handle("float32") = T.tvm_struct_get(A_red, 0, 1, "handle")
        T.attr(A_red_1, "storage_alignment", 64)
        sum_A_red_shape: T.handle("int64") = T.tvm_struct_get(A_red, 0, 2, "handle")
        sum_A_red_shape_1 = T.decl_buffer((1,), "int64", data=sum_A_red_shape)
        sum_A_red_strides: T.handle("int64") = T.tvm_struct_get(A_red, 0, 3, "handle")
        sum_A_red_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_red_strides)
        assert A_code == 3 or A_code == 13 or A_code == 7 or A_code == 4, "sum: Expect arg[0] to be pointer"
        assert A_red_code == 3 or A_red_code == 13 or A_red_code == 7 or A_red_code == 4, "sum: Expect arg[1] to be pointer"
        T.attr("default", "device_id", dev_id)
        T.attr("default", "device_type", 2)
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert T.tvm_struct_get(A, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A, 0, 7, "uint16") == T.uint16(1), "sum.A.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_shape_1[0]) == 16, "Argument sum.A.shape[0] has an unsatisfied constraint: 16 == T.Cast(\"int32\", sum_A_shape[0])"
        assert T.Cast("int32", sum_A_shape_1[1]) == 4, "Argument sum.A.shape[1] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_shape[1])"
        assert T.Cast("int32", sum_A_shape_1[2]) == 3, "Argument sum.A.shape[2] has an unsatisfied constraint: 3 == T.Cast(\"int32\", sum_A_shape[2])"
        if not T.isnullptr(sum_A_strides):
            assert 1 == T.Cast("int32", sum_A_strides_1[2]) and 3 == T.Cast("int32", sum_A_strides_1[1]) and 12 == T.Cast("int32", sum_A_strides_1[0]), "sum.A.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A, 0, 8, "uint64"), "Argument sum.A.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A, 0, 10, "int32") == 2, "Argument sum.A.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A, 0, 10, \"int32\")"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert T.tvm_struct_get(A_red, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A_red, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A_red, 0, 7, "uint16") == T.uint16(1), "sum.A_red.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_red_shape_1[0]) == 4, "Argument sum.A_red.shape[0] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_red_shape[0])"
        if not T.isnullptr(sum_A_red_strides):
            assert 1 == T.Cast("int32", sum_A_red_strides_1[0]), "sum.A_red.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, "uint64"), "Argument sum.A_red.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A_red, 0, 10, "int32") == 2, "Argument sum.A_red.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A_red, 0, 10, \"int32\")"
        assert dev_id == T.tvm_struct_get(A_red, 0, 9, "int32"), "Argument sum.A_red.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(A_red, 0, 9, \"int32\")"
        A_2 = T.decl_buffer((16, 4, 3), data=A_1)
        A_red_2 = T.decl_buffer((4,), data=A_red_1)
        T.call_packed("__tvm_set_device", 2, dev_id)
        with T.attr(0, "compute_scope", "sum_compute_"):
            T.call_packed("sum_kernel", A_1, A_red_1, 1, 32, 32)
        T.ret(0)

    @T.prim_func
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"calling_conv": 2, "target": T.target({"arch": "sm_80", "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.kernel_launch_params": ["blockIdx.x", "threadIdx.y", "threadIdx.x"], "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tvm_warp_activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 16, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 8, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 4, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 2, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 1, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tvm_warp_shuffle(mask_1[0], red_buf0_1[0], 32 * threadIdx_y, 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: sequential
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.Filter
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.Filter
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(args: T.handle, arg_type_ids: T.handle("int32"), num_args: T.int32, out_ret_value: T.handle("void"), out_ret_tcode: T.handle("int32"), resource_handle: T.handle) -> T.int32:
        T.func_attr({"calling_conv": 1, "from_legacy_te_schedule": T.bool(True), "target": T.target({"keys": ["cpu"], "kind": "llvm", "tag": ""}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        assert num_args == 2, "sum: num_args should be 2"
        arg_type_ids_1 = T.decl_buffer((2,), "int32", data=arg_type_ids)
        A_code: T.int32 = arg_type_ids_1[0]
        A_red_code: T.int32 = arg_type_ids_1[1]
        A: T.handle = T.tvm_struct_get(args, 0, 12, "handle")
        A_red: T.handle = T.tvm_struct_get(args, 1, 12, "handle")
        A_1: T.handle("float32") = T.tvm_struct_get(A, 0, 1, "handle")
        T.attr(A_1, "storage_alignment", 64)
        sum_A_shape: T.handle("int64") = T.tvm_struct_get(A, 0, 2, "handle")
        sum_A_shape_1 = T.decl_buffer((3,), "int64", data=sum_A_shape)
        sum_A_strides: T.handle("int64") = T.tvm_struct_get(A, 0, 3, "handle")
        sum_A_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_strides)
        dev_id: T.int32 = T.tvm_struct_get(A, 0, 9, "int32")
        A_red_1: T.handle("float32") = T.tvm_struct_get(A_red, 0, 1, "handle")
        T.attr(A_red_1, "storage_alignment", 64)
        sum_A_red_shape: T.handle("int64") = T.tvm_struct_get(A_red, 0, 2, "handle")
        sum_A_red_shape_1 = T.decl_buffer((1,), "int64", data=sum_A_red_shape)
        sum_A_red_strides: T.handle("int64") = T.tvm_struct_get(A_red, 0, 3, "handle")
        sum_A_red_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_red_strides)
        assert A_code == 3 or A_code == 13 or A_code == 7 or A_code == 4, "sum: Expect arg[0] to be pointer"
        assert A_red_code == 3 or A_red_code == 13 or A_red_code == 7 or A_red_code == 4, "sum: Expect arg[1] to be pointer"
        T.attr("default", "device_id", dev_id)
        T.attr("default", "device_type", 2)
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert T.tvm_struct_get(A, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A, 0, 7, "uint16") == T.uint16(1), "sum.A.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_shape_1[0]) == 16, "Argument sum.A.shape[0] has an unsatisfied constraint: 16 == T.Cast(\"int32\", sum_A_shape[0])"
        assert T.Cast("int32", sum_A_shape_1[1]) == 4, "Argument sum.A.shape[1] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_shape[1])"
        assert T.Cast("int32", sum_A_shape_1[2]) == 3, "Argument sum.A.shape[2] has an unsatisfied constraint: 3 == T.Cast(\"int32\", sum_A_shape[2])"
        if not T.isnullptr(sum_A_strides):
            assert 1 == T.Cast("int32", sum_A_strides_1[2]) and 3 == T.Cast("int32", sum_A_strides_1[1]) and 12 == T.Cast("int32", sum_A_strides_1[0]), "sum.A.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A, 0, 8, "uint64"), "Argument sum.A.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A, 0, 10, "int32") == 2, "Argument sum.A.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A, 0, 10, \"int32\")"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert T.tvm_struct_get(A_red, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A_red, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A_red, 0, 7, "uint16") == T.uint16(1), "sum.A_red.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_red_shape_1[0]) == 4, "Argument sum.A_red.shape[0] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_red_shape[0])"
        if not T.isnullptr(sum_A_red_strides):
            assert 1 == T.Cast("int32", sum_A_red_strides_1[0]), "sum.A_red.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, "uint64"), "Argument sum.A_red.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A_red, 0, 10, "int32") == 2, "Argument sum.A_red.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A_red, 0, 10, \"int32\")"
        assert dev_id == T.tvm_struct_get(A_red, 0, 9, "int32"), "Argument sum.A_red.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(A_red, 0, 9, \"int32\")"
        A_2 = T.decl_buffer((16, 4, 3), data=A_1)
        A_red_2 = T.decl_buffer((4,), data=A_red_1)
        T.call_packed("__tvm_set_device", 2, dev_id)
        with T.attr(0, "compute_scope", "sum_compute_"):
            T.call_packed("sum_kernel", A_1, A_red_1, 1, 32, 32)
        T.ret(0)
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.BindTarget
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.BindTarget
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(args: T.handle, arg_type_ids: T.handle("int32"), num_args: T.int32, out_ret_value: T.handle("void"), out_ret_tcode: T.handle("int32"), resource_handle: T.handle) -> T.int32:
        T.func_attr({"calling_conv": 1, "from_legacy_te_schedule": T.bool(True), "target": T.target({"keys": ["cpu"], "kind": "llvm", "tag": ""}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        assert num_args == 2, "sum: num_args should be 2"
        arg_type_ids_1 = T.decl_buffer((2,), "int32", data=arg_type_ids)
        A_code: T.int32 = arg_type_ids_1[0]
        A_red_code: T.int32 = arg_type_ids_1[1]
        A: T.handle = T.tvm_struct_get(args, 0, 12, "handle")
        A_red: T.handle = T.tvm_struct_get(args, 1, 12, "handle")
        A_1: T.handle("float32") = T.tvm_struct_get(A, 0, 1, "handle")
        T.attr(A_1, "storage_alignment", 64)
        sum_A_shape: T.handle("int64") = T.tvm_struct_get(A, 0, 2, "handle")
        sum_A_shape_1 = T.decl_buffer((3,), "int64", data=sum_A_shape)
        sum_A_strides: T.handle("int64") = T.tvm_struct_get(A, 0, 3, "handle")
        sum_A_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_strides)
        dev_id: T.int32 = T.tvm_struct_get(A, 0, 9, "int32")
        A_red_1: T.handle("float32") = T.tvm_struct_get(A_red, 0, 1, "handle")
        T.attr(A_red_1, "storage_alignment", 64)
        sum_A_red_shape: T.handle("int64") = T.tvm_struct_get(A_red, 0, 2, "handle")
        sum_A_red_shape_1 = T.decl_buffer((1,), "int64", data=sum_A_red_shape)
        sum_A_red_strides: T.handle("int64") = T.tvm_struct_get(A_red, 0, 3, "handle")
        sum_A_red_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_red_strides)
        assert A_code == 3 or A_code == 13 or A_code == 7 or A_code == 4, "sum: Expect arg[0] to be pointer"
        assert A_red_code == 3 or A_red_code == 13 or A_red_code == 7 or A_red_code == 4, "sum: Expect arg[1] to be pointer"
        T.attr("default", "device_id", dev_id)
        T.attr("default", "device_type", 2)
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert T.tvm_struct_get(A, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A, 0, 7, "uint16") == T.uint16(1), "sum.A.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_shape_1[0]) == 16, "Argument sum.A.shape[0] has an unsatisfied constraint: 16 == T.Cast(\"int32\", sum_A_shape[0])"
        assert T.Cast("int32", sum_A_shape_1[1]) == 4, "Argument sum.A.shape[1] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_shape[1])"
        assert T.Cast("int32", sum_A_shape_1[2]) == 3, "Argument sum.A.shape[2] has an unsatisfied constraint: 3 == T.Cast(\"int32\", sum_A_shape[2])"
        if not T.isnullptr(sum_A_strides):
            assert 1 == T.Cast("int32", sum_A_strides_1[2]) and 3 == T.Cast("int32", sum_A_strides_1[1]) and 12 == T.Cast("int32", sum_A_strides_1[0]), "sum.A.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A, 0, 8, "uint64"), "Argument sum.A.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A, 0, 10, "int32") == 2, "Argument sum.A.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A, 0, 10, \"int32\")"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert T.tvm_struct_get(A_red, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A_red, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A_red, 0, 7, "uint16") == T.uint16(1), "sum.A_red.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_red_shape_1[0]) == 4, "Argument sum.A_red.shape[0] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_red_shape[0])"
        if not T.isnullptr(sum_A_red_strides):
            assert 1 == T.Cast("int32", sum_A_red_strides_1[0]), "sum.A_red.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, "uint64"), "Argument sum.A_red.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A_red, 0, 10, "int32") == 2, "Argument sum.A_red.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A_red, 0, 10, \"int32\")"
        assert dev_id == T.tvm_struct_get(A_red, 0, 9, "int32"), "Argument sum.A_red.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(A_red, 0, 9, \"int32\")"
        A_2 = T.decl_buffer((16, 4, 3), data=A_1)
        A_red_2 = T.decl_buffer((4,), data=A_red_1)
        T.call_packed("__tvm_set_device", 2, dev_id)
        with T.attr(0, "compute_scope", "sum_compute_"):
            T.call_packed("sum_kernel", A_1, A_red_1, 1, 32, 32)
        T.ret(0)
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LowerTVMBuiltin
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LowerTVMBuiltin
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(args: T.handle, arg_type_ids: T.handle("int32"), num_args: T.int32, out_ret_value: T.handle("void"), out_ret_tcode: T.handle("int32"), resource_handle: T.handle) -> T.int32:
        T.func_attr({"calling_conv": 1, "from_legacy_te_schedule": T.bool(True), "target": T.target({"keys": ["cpu"], "kind": "llvm", "tag": ""}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        stack_tcode: T.handle("int32") = T.tvm_stack_alloca("arg_tcode", 6)
        stack_tcode_1 = T.decl_buffer((T.uint64(6),), "int32", data=stack_tcode)
        stack_value: T.handle = T.tvm_stack_alloca("arg_value", 6)
        assert num_args == 2, "sum: num_args should be 2"
        arg_type_ids_1 = T.decl_buffer((2,), "int32", data=arg_type_ids)
        A_code: T.int32 = arg_type_ids_1[0]
        A_red_code: T.int32 = arg_type_ids_1[1]
        A: T.handle = T.tvm_struct_get(args, 0, 12, "handle")
        A_red: T.handle = T.tvm_struct_get(args, 1, 12, "handle")
        A_1: T.handle("float32") = T.tvm_struct_get(A, 0, 1, "handle")
        T.attr(A_1, "storage_alignment", 64)
        sum_A_shape: T.handle("int64") = T.tvm_struct_get(A, 0, 2, "handle")
        sum_A_shape_1 = T.decl_buffer((3,), "int64", data=sum_A_shape)
        sum_A_strides: T.handle("int64") = T.tvm_struct_get(A, 0, 3, "handle")
        sum_A_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_strides)
        dev_id: T.int32 = T.tvm_struct_get(A, 0, 9, "int32")
        A_red_1: T.handle("float32") = T.tvm_struct_get(A_red, 0, 1, "handle")
        T.attr(A_red_1, "storage_alignment", 64)
        sum_A_red_shape: T.handle("int64") = T.tvm_struct_get(A_red, 0, 2, "handle")
        sum_A_red_shape_1 = T.decl_buffer((1,), "int64", data=sum_A_red_shape)
        sum_A_red_strides: T.handle("int64") = T.tvm_struct_get(A_red, 0, 3, "handle")
        sum_A_red_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_red_strides)
        assert A_code == 3 or A_code == 13 or A_code == 7 or A_code == 4, "sum: Expect arg[0] to be pointer"
        assert A_red_code == 3 or A_red_code == 13 or A_red_code == 7 or A_red_code == 4, "sum: Expect arg[1] to be pointer"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert T.tvm_struct_get(A, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A, 0, 7, "uint16") == T.uint16(1), "sum.A.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_shape_1[0]) == 16, "Argument sum.A.shape[0] has an unsatisfied constraint: 16 == T.Cast(\"int32\", sum_A_shape[0])"
        assert T.Cast("int32", sum_A_shape_1[1]) == 4, "Argument sum.A.shape[1] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_shape[1])"
        assert T.Cast("int32", sum_A_shape_1[2]) == 3, "Argument sum.A.shape[2] has an unsatisfied constraint: 3 == T.Cast(\"int32\", sum_A_shape[2])"
        if not T.isnullptr(sum_A_strides):
            assert 1 == T.Cast("int32", sum_A_strides_1[2]) and 3 == T.Cast("int32", sum_A_strides_1[1]) and 12 == T.Cast("int32", sum_A_strides_1[0]), "sum.A.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A, 0, 8, "uint64"), "Argument sum.A.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A, 0, 10, "int32") == 2, "Argument sum.A.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A, 0, 10, \"int32\")"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert T.tvm_struct_get(A_red, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A_red, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A_red, 0, 7, "uint16") == T.uint16(1), "sum.A_red.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_red_shape_1[0]) == 4, "Argument sum.A_red.shape[0] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_red_shape[0])"
        if not T.isnullptr(sum_A_red_strides):
            assert 1 == T.Cast("int32", sum_A_red_strides_1[0]), "sum.A_red.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, "uint64"), "Argument sum.A_red.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A_red, 0, 10, "int32") == 2, "Argument sum.A_red.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A_red, 0, 10, \"int32\")"
        assert dev_id == T.tvm_struct_get(A_red, 0, 9, "int32"), "Argument sum.A_red.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(A_red, 0, 9, \"int32\")"
        A_2 = T.decl_buffer((16, 4, 3), data=A_1)
        A_red_2 = T.decl_buffer((4,), data=A_red_1)
        T.tvm_struct_set(stack_value, 0, 12, T.Cast("int64", 2))
        stack_tcode_1[0] = 0
        T.tvm_struct_set(stack_value, 1, 12, T.Cast("int64", dev_id))
        stack_tcode_1[1] = 0
        T.call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2)
        with T.attr(0, "compute_scope", "sum_compute_"):
            T.tvm_struct_set(stack_value, 0, 12, A_1)
            if T.isnullptr(A_1):
                stack_tcode_1[0] = 4
            else:
                stack_tcode_1[0] = 3
            T.tvm_struct_set(stack_value, 1, 12, A_red_1)
            if T.isnullptr(A_red_1):
                stack_tcode_1[1] = 4
            else:
                stack_tcode_1[1] = 3
            T.tvm_struct_set(stack_value, 2, 12, T.Cast("int64", 1))
            stack_tcode_1[2] = 0
            T.tvm_struct_set(stack_value, 3, 12, T.Cast("int64", 32))
            stack_tcode_1[3] = 0
            T.tvm_struct_set(stack_value, 4, 12, T.Cast("int64", 32))
            stack_tcode_1[4] = 0
            T.call_packed_lowered("sum_kernel", stack_value, stack_tcode, 0, 5)
        T.ret(0)
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LowerCustomDatatypes
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LowerCustomDatatypes
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(args: T.handle, arg_type_ids: T.handle("int32"), num_args: T.int32, out_ret_value: T.handle("void"), out_ret_tcode: T.handle("int32"), resource_handle: T.handle) -> T.int32:
        T.func_attr({"calling_conv": 1, "from_legacy_te_schedule": T.bool(True), "target": T.target({"keys": ["cpu"], "kind": "llvm", "tag": ""}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        stack_tcode: T.handle("int32") = T.tvm_stack_alloca("arg_tcode", 6)
        stack_tcode_1 = T.decl_buffer((T.uint64(6),), "int32", data=stack_tcode)
        stack_value: T.handle = T.tvm_stack_alloca("arg_value", 6)
        assert num_args == 2, "sum: num_args should be 2"
        arg_type_ids_1 = T.decl_buffer((2,), "int32", data=arg_type_ids)
        A_code: T.int32 = arg_type_ids_1[0]
        A_red_code: T.int32 = arg_type_ids_1[1]
        A: T.handle = T.tvm_struct_get(args, 0, 12, "handle")
        A_red: T.handle = T.tvm_struct_get(args, 1, 12, "handle")
        A_1: T.handle("float32") = T.tvm_struct_get(A, 0, 1, "handle")
        T.attr(A_1, "storage_alignment", 64)
        sum_A_shape: T.handle("int64") = T.tvm_struct_get(A, 0, 2, "handle")
        sum_A_shape_1 = T.decl_buffer((3,), "int64", data=sum_A_shape)
        sum_A_strides: T.handle("int64") = T.tvm_struct_get(A, 0, 3, "handle")
        sum_A_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_strides)
        dev_id: T.int32 = T.tvm_struct_get(A, 0, 9, "int32")
        A_red_1: T.handle("float32") = T.tvm_struct_get(A_red, 0, 1, "handle")
        T.attr(A_red_1, "storage_alignment", 64)
        sum_A_red_shape: T.handle("int64") = T.tvm_struct_get(A_red, 0, 2, "handle")
        sum_A_red_shape_1 = T.decl_buffer((1,), "int64", data=sum_A_red_shape)
        sum_A_red_strides: T.handle("int64") = T.tvm_struct_get(A_red, 0, 3, "handle")
        sum_A_red_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_red_strides)
        assert A_code == 3 or A_code == 13 or A_code == 7 or A_code == 4, "sum: Expect arg[0] to be pointer"
        assert A_red_code == 3 or A_red_code == 13 or A_red_code == 7 or A_red_code == 4, "sum: Expect arg[1] to be pointer"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert T.tvm_struct_get(A, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A, 0, 7, "uint16") == T.uint16(1), "sum.A.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_shape_1[0]) == 16, "Argument sum.A.shape[0] has an unsatisfied constraint: 16 == T.Cast(\"int32\", sum_A_shape[0])"
        assert T.Cast("int32", sum_A_shape_1[1]) == 4, "Argument sum.A.shape[1] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_shape[1])"
        assert T.Cast("int32", sum_A_shape_1[2]) == 3, "Argument sum.A.shape[2] has an unsatisfied constraint: 3 == T.Cast(\"int32\", sum_A_shape[2])"
        if not T.isnullptr(sum_A_strides):
            assert 1 == T.Cast("int32", sum_A_strides_1[2]) and 3 == T.Cast("int32", sum_A_strides_1[1]) and 12 == T.Cast("int32", sum_A_strides_1[0]), "sum.A.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A, 0, 8, "uint64"), "Argument sum.A.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A, 0, 10, "int32") == 2, "Argument sum.A.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A, 0, 10, \"int32\")"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert T.tvm_struct_get(A_red, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A_red, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A_red, 0, 7, "uint16") == T.uint16(1), "sum.A_red.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_red_shape_1[0]) == 4, "Argument sum.A_red.shape[0] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_red_shape[0])"
        if not T.isnullptr(sum_A_red_strides):
            assert 1 == T.Cast("int32", sum_A_red_strides_1[0]), "sum.A_red.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, "uint64"), "Argument sum.A_red.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A_red, 0, 10, "int32") == 2, "Argument sum.A_red.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A_red, 0, 10, \"int32\")"
        assert dev_id == T.tvm_struct_get(A_red, 0, 9, "int32"), "Argument sum.A_red.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(A_red, 0, 9, \"int32\")"
        A_2 = T.decl_buffer((16, 4, 3), data=A_1)
        A_red_2 = T.decl_buffer((4,), data=A_red_1)
        T.tvm_struct_set(stack_value, 0, 12, T.Cast("int64", 2))
        stack_tcode_1[0] = 0
        T.tvm_struct_set(stack_value, 1, 12, T.Cast("int64", dev_id))
        stack_tcode_1[1] = 0
        T.call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2)
        with T.attr(0, "compute_scope", "sum_compute_"):
            T.tvm_struct_set(stack_value, 0, 12, A_1)
            if T.isnullptr(A_1):
                stack_tcode_1[0] = 4
            else:
                stack_tcode_1[0] = 3
            T.tvm_struct_set(stack_value, 1, 12, A_red_1)
            if T.isnullptr(A_red_1):
                stack_tcode_1[1] = 4
            else:
                stack_tcode_1[1] = 3
            T.tvm_struct_set(stack_value, 2, 12, T.Cast("int64", 1))
            stack_tcode_1[2] = 0
            T.tvm_struct_set(stack_value, 3, 12, T.Cast("int64", 32))
            stack_tcode_1[3] = 0
            T.tvm_struct_set(stack_value, 4, 12, T.Cast("int64", 32))
            stack_tcode_1[4] = 0
            T.call_packed_lowered("sum_kernel", stack_value, stack_tcode, 0, 5)
        T.ret(0)
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LowerIntrin
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LowerIntrin
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(args: T.handle, arg_type_ids: T.handle("int32"), num_args: T.int32, out_ret_value: T.handle("void"), out_ret_tcode: T.handle("int32"), resource_handle: T.handle) -> T.int32:
        T.func_attr({"calling_conv": 1, "from_legacy_te_schedule": T.bool(True), "target": T.target({"keys": ["cpu"], "kind": "llvm", "tag": ""}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        stack_tcode: T.handle("int32") = T.tvm_stack_alloca("arg_tcode", 6)
        stack_tcode_1 = T.decl_buffer((T.uint64(6),), "int32", data=stack_tcode)
        stack_value: T.handle = T.tvm_stack_alloca("arg_value", 6)
        assert num_args == 2, "sum: num_args should be 2"
        arg_type_ids_1 = T.decl_buffer((2,), "int32", data=arg_type_ids)
        A_code: T.int32 = arg_type_ids_1[0]
        A_red_code: T.int32 = arg_type_ids_1[1]
        A: T.handle = T.tvm_struct_get(args, 0, 12, "handle")
        A_red: T.handle = T.tvm_struct_get(args, 1, 12, "handle")
        A_1: T.handle("float32") = T.tvm_struct_get(A, 0, 1, "handle")
        T.attr(A_1, "storage_alignment", 64)
        sum_A_shape: T.handle("int64") = T.tvm_struct_get(A, 0, 2, "handle")
        sum_A_shape_1 = T.decl_buffer((3,), "int64", data=sum_A_shape)
        sum_A_strides: T.handle("int64") = T.tvm_struct_get(A, 0, 3, "handle")
        sum_A_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_strides)
        dev_id: T.int32 = T.tvm_struct_get(A, 0, 9, "int32")
        A_red_1: T.handle("float32") = T.tvm_struct_get(A_red, 0, 1, "handle")
        T.attr(A_red_1, "storage_alignment", 64)
        sum_A_red_shape: T.handle("int64") = T.tvm_struct_get(A_red, 0, 2, "handle")
        sum_A_red_shape_1 = T.decl_buffer((1,), "int64", data=sum_A_red_shape)
        sum_A_red_strides: T.handle("int64") = T.tvm_struct_get(A_red, 0, 3, "handle")
        sum_A_red_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_red_strides)
        assert A_code == 3 or A_code == 13 or A_code == 7 or A_code == 4, "sum: Expect arg[0] to be pointer"
        assert A_red_code == 3 or A_red_code == 13 or A_red_code == 7 or A_red_code == 4, "sum: Expect arg[1] to be pointer"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert T.tvm_struct_get(A, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A, 0, 7, "uint16") == T.uint16(1), "sum.A.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_shape_1[0]) == 16, "Argument sum.A.shape[0] has an unsatisfied constraint: 16 == T.Cast(\"int32\", sum_A_shape[0])"
        assert T.Cast("int32", sum_A_shape_1[1]) == 4, "Argument sum.A.shape[1] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_shape[1])"
        assert T.Cast("int32", sum_A_shape_1[2]) == 3, "Argument sum.A.shape[2] has an unsatisfied constraint: 3 == T.Cast(\"int32\", sum_A_shape[2])"
        if not T.isnullptr(sum_A_strides):
            assert 1 == T.Cast("int32", sum_A_strides_1[2]) and 3 == T.Cast("int32", sum_A_strides_1[1]) and 12 == T.Cast("int32", sum_A_strides_1[0]), "sum.A.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A, 0, 8, "uint64"), "Argument sum.A.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A, 0, 10, "int32") == 2, "Argument sum.A.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A, 0, 10, \"int32\")"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert T.tvm_struct_get(A_red, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A_red, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A_red, 0, 7, "uint16") == T.uint16(1), "sum.A_red.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_red_shape_1[0]) == 4, "Argument sum.A_red.shape[0] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_red_shape[0])"
        if not T.isnullptr(sum_A_red_strides):
            assert 1 == T.Cast("int32", sum_A_red_strides_1[0]), "sum.A_red.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, "uint64"), "Argument sum.A_red.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A_red, 0, 10, "int32") == 2, "Argument sum.A_red.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A_red, 0, 10, \"int32\")"
        assert dev_id == T.tvm_struct_get(A_red, 0, 9, "int32"), "Argument sum.A_red.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(A_red, 0, 9, \"int32\")"
        A_2 = T.decl_buffer((16, 4, 3), data=A_1)
        A_red_2 = T.decl_buffer((4,), data=A_red_1)
        T.tvm_struct_set(stack_value, 0, 12, T.Cast("int64", 2))
        stack_tcode_1[0] = 0
        T.tvm_struct_set(stack_value, 1, 12, T.Cast("int64", dev_id))
        stack_tcode_1[1] = 0
        T.call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2)
        with T.attr(0, "compute_scope", "sum_compute_"):
            T.tvm_struct_set(stack_value, 0, 12, A_1)
            if T.isnullptr(A_1):
                stack_tcode_1[0] = 4
            else:
                stack_tcode_1[0] = 3
            T.tvm_struct_set(stack_value, 1, 12, A_red_1)
            if T.isnullptr(A_red_1):
                stack_tcode_1[1] = 4
            else:
                stack_tcode_1[1] = 3
            T.tvm_struct_set(stack_value, 2, 12, T.Cast("int64", 1))
            stack_tcode_1[2] = 0
            T.tvm_struct_set(stack_value, 3, 12, T.Cast("int64", 32))
            stack_tcode_1[3] = 0
            T.tvm_struct_set(stack_value, 4, 12, T.Cast("int64", 32))
            stack_tcode_1[4] = 0
            T.call_packed_lowered("sum_kernel", stack_value, stack_tcode, 0, 5)
        T.ret(0)
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LowerDeviceStorageAccessInfo
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LowerDeviceStorageAccessInfo
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(args: T.handle, arg_type_ids: T.handle("int32"), num_args: T.int32, out_ret_value: T.handle("void"), out_ret_tcode: T.handle("int32"), resource_handle: T.handle) -> T.int32:
        T.func_attr({"calling_conv": 1, "from_legacy_te_schedule": T.bool(True), "target": T.target({"keys": ["cpu"], "kind": "llvm", "tag": ""}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        stack_tcode: T.handle("int32") = T.tvm_stack_alloca("arg_tcode", 6)
        stack_tcode_1 = T.decl_buffer((T.uint64(6),), "int32", data=stack_tcode)
        stack_value: T.handle = T.tvm_stack_alloca("arg_value", 6)
        assert num_args == 2, "sum: num_args should be 2"
        arg_type_ids_1 = T.decl_buffer((2,), "int32", data=arg_type_ids)
        A_code: T.int32 = arg_type_ids_1[0]
        A_red_code: T.int32 = arg_type_ids_1[1]
        A: T.handle = T.tvm_struct_get(args, 0, 12, "handle")
        A_red: T.handle = T.tvm_struct_get(args, 1, 12, "handle")
        A_1: T.handle("float32") = T.tvm_struct_get(A, 0, 1, "handle")
        T.attr(A_1, "storage_alignment", 64)
        sum_A_shape: T.handle("int64") = T.tvm_struct_get(A, 0, 2, "handle")
        sum_A_shape_1 = T.decl_buffer((3,), "int64", data=sum_A_shape)
        sum_A_strides: T.handle("int64") = T.tvm_struct_get(A, 0, 3, "handle")
        sum_A_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_strides)
        dev_id: T.int32 = T.tvm_struct_get(A, 0, 9, "int32")
        A_red_1: T.handle("float32") = T.tvm_struct_get(A_red, 0, 1, "handle")
        T.attr(A_red_1, "storage_alignment", 64)
        sum_A_red_shape: T.handle("int64") = T.tvm_struct_get(A_red, 0, 2, "handle")
        sum_A_red_shape_1 = T.decl_buffer((1,), "int64", data=sum_A_red_shape)
        sum_A_red_strides: T.handle("int64") = T.tvm_struct_get(A_red, 0, 3, "handle")
        sum_A_red_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_red_strides)
        assert A_code == 3 or A_code == 13 or A_code == 7 or A_code == 4, "sum: Expect arg[0] to be pointer"
        assert A_red_code == 3 or A_red_code == 13 or A_red_code == 7 or A_red_code == 4, "sum: Expect arg[1] to be pointer"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert T.tvm_struct_get(A, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A, 0, 7, "uint16") == T.uint16(1), "sum.A.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_shape_1[0]) == 16, "Argument sum.A.shape[0] has an unsatisfied constraint: 16 == T.Cast(\"int32\", sum_A_shape[0])"
        assert T.Cast("int32", sum_A_shape_1[1]) == 4, "Argument sum.A.shape[1] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_shape[1])"
        assert T.Cast("int32", sum_A_shape_1[2]) == 3, "Argument sum.A.shape[2] has an unsatisfied constraint: 3 == T.Cast(\"int32\", sum_A_shape[2])"
        if not T.isnullptr(sum_A_strides):
            assert 1 == T.Cast("int32", sum_A_strides_1[2]) and 3 == T.Cast("int32", sum_A_strides_1[1]) and 12 == T.Cast("int32", sum_A_strides_1[0]), "sum.A.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A, 0, 8, "uint64"), "Argument sum.A.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A, 0, 10, "int32") == 2, "Argument sum.A.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A, 0, 10, \"int32\")"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert T.tvm_struct_get(A_red, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A_red, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A_red, 0, 7, "uint16") == T.uint16(1), "sum.A_red.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_red_shape_1[0]) == 4, "Argument sum.A_red.shape[0] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_red_shape[0])"
        if not T.isnullptr(sum_A_red_strides):
            assert 1 == T.Cast("int32", sum_A_red_strides_1[0]), "sum.A_red.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, "uint64"), "Argument sum.A_red.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A_red, 0, 10, "int32") == 2, "Argument sum.A_red.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A_red, 0, 10, \"int32\")"
        assert dev_id == T.tvm_struct_get(A_red, 0, 9, "int32"), "Argument sum.A_red.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(A_red, 0, 9, \"int32\")"
        A_2 = T.decl_buffer((16, 4, 3), data=A_1)
        A_red_2 = T.decl_buffer((4,), data=A_red_1)
        T.tvm_struct_set(stack_value, 0, 12, T.Cast("int64", 2))
        stack_tcode_1[0] = 0
        T.tvm_struct_set(stack_value, 1, 12, T.Cast("int64", dev_id))
        stack_tcode_1[1] = 0
        T.call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2)
        with T.attr(0, "compute_scope", "sum_compute_"):
            T.tvm_struct_set(stack_value, 0, 12, A_1)
            if T.isnullptr(A_1):
                stack_tcode_1[0] = 4
            else:
                stack_tcode_1[0] = 3
            T.tvm_struct_set(stack_value, 1, 12, A_red_1)
            if T.isnullptr(A_red_1):
                stack_tcode_1[1] = 4
            else:
                stack_tcode_1[1] = 3
            T.tvm_struct_set(stack_value, 2, 12, T.Cast("int64", 1))
            stack_tcode_1[2] = 0
            T.tvm_struct_set(stack_value, 3, 12, T.Cast("int64", 32))
            stack_tcode_1[3] = 0
            T.tvm_struct_set(stack_value, 4, 12, T.Cast("int64", 32))
            stack_tcode_1[4] = 0
            T.call_packed_lowered("sum_kernel", stack_value, stack_tcode, 0, 5)
        T.ret(0)
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.CombineContextCall
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.CombineContextCall
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(args: T.handle, arg_type_ids: T.handle("int32"), num_args: T.int32, out_ret_value: T.handle("void"), out_ret_tcode: T.handle("int32"), resource_handle: T.handle) -> T.int32:
        T.func_attr({"calling_conv": 1, "from_legacy_te_schedule": T.bool(True), "target": T.target({"keys": ["cpu"], "kind": "llvm", "tag": ""}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        stack_tcode: T.handle("int32") = T.tvm_stack_alloca("arg_tcode", 6)
        stack_tcode_1 = T.decl_buffer((T.uint64(6),), "int32", data=stack_tcode)
        stack_value: T.handle = T.tvm_stack_alloca("arg_value", 6)
        assert num_args == 2, "sum: num_args should be 2"
        arg_type_ids_1 = T.decl_buffer((2,), "int32", data=arg_type_ids)
        A_code: T.int32 = arg_type_ids_1[0]
        A_red_code: T.int32 = arg_type_ids_1[1]
        A: T.handle = T.tvm_struct_get(args, 0, 12, "handle")
        A_red: T.handle = T.tvm_struct_get(args, 1, 12, "handle")
        A_1: T.handle("float32") = T.tvm_struct_get(A, 0, 1, "handle")
        T.attr(A_1, "storage_alignment", 64)
        sum_A_shape: T.handle("int64") = T.tvm_struct_get(A, 0, 2, "handle")
        sum_A_shape_1 = T.decl_buffer((3,), "int64", data=sum_A_shape)
        sum_A_strides: T.handle("int64") = T.tvm_struct_get(A, 0, 3, "handle")
        sum_A_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_strides)
        dev_id: T.int32 = T.tvm_struct_get(A, 0, 9, "int32")
        A_red_1: T.handle("float32") = T.tvm_struct_get(A_red, 0, 1, "handle")
        T.attr(A_red_1, "storage_alignment", 64)
        sum_A_red_shape: T.handle("int64") = T.tvm_struct_get(A_red, 0, 2, "handle")
        sum_A_red_shape_1 = T.decl_buffer((1,), "int64", data=sum_A_red_shape)
        sum_A_red_strides: T.handle("int64") = T.tvm_struct_get(A_red, 0, 3, "handle")
        sum_A_red_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_red_strides)
        assert A_code == 3 or A_code == 13 or A_code == 7 or A_code == 4, "sum: Expect arg[0] to be pointer"
        assert A_red_code == 3 or A_red_code == 13 or A_red_code == 7 or A_red_code == 4, "sum: Expect arg[1] to be pointer"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert T.tvm_struct_get(A, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A, 0, 7, "uint16") == T.uint16(1), "sum.A.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_shape_1[0]) == 16, "Argument sum.A.shape[0] has an unsatisfied constraint: 16 == T.Cast(\"int32\", sum_A_shape[0])"
        assert T.Cast("int32", sum_A_shape_1[1]) == 4, "Argument sum.A.shape[1] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_shape[1])"
        assert T.Cast("int32", sum_A_shape_1[2]) == 3, "Argument sum.A.shape[2] has an unsatisfied constraint: 3 == T.Cast(\"int32\", sum_A_shape[2])"
        if not T.isnullptr(sum_A_strides):
            assert 1 == T.Cast("int32", sum_A_strides_1[2]) and 3 == T.Cast("int32", sum_A_strides_1[1]) and 12 == T.Cast("int32", sum_A_strides_1[0]), "sum.A.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A, 0, 8, "uint64"), "Argument sum.A.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A, 0, 10, "int32") == 2, "Argument sum.A.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A, 0, 10, \"int32\")"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert T.tvm_struct_get(A_red, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A_red, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A_red, 0, 7, "uint16") == T.uint16(1), "sum.A_red.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_red_shape_1[0]) == 4, "Argument sum.A_red.shape[0] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_red_shape[0])"
        if not T.isnullptr(sum_A_red_strides):
            assert 1 == T.Cast("int32", sum_A_red_strides_1[0]), "sum.A_red.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, "uint64"), "Argument sum.A_red.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A_red, 0, 10, "int32") == 2, "Argument sum.A_red.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A_red, 0, 10, \"int32\")"
        assert dev_id == T.tvm_struct_get(A_red, 0, 9, "int32"), "Argument sum.A_red.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(A_red, 0, 9, \"int32\")"
        A_2 = T.decl_buffer((16, 4, 3), data=A_1)
        A_red_2 = T.decl_buffer((4,), data=A_red_1)
        T.tvm_struct_set(stack_value, 0, 12, T.Cast("int64", 2))
        stack_tcode_1[0] = 0
        T.tvm_struct_set(stack_value, 1, 12, T.Cast("int64", dev_id))
        stack_tcode_1[1] = 0
        T.call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2)
        with T.attr(0, "compute_scope", "sum_compute_"):
            T.tvm_struct_set(stack_value, 0, 12, A_1)
            if T.isnullptr(A_1):
                stack_tcode_1[0] = 4
            else:
                stack_tcode_1[0] = 3
            T.tvm_struct_set(stack_value, 1, 12, A_red_1)
            if T.isnullptr(A_red_1):
                stack_tcode_1[1] = 4
            else:
                stack_tcode_1[1] = 3
            T.tvm_struct_set(stack_value, 2, 12, T.Cast("int64", 1))
            stack_tcode_1[2] = 0
            T.tvm_struct_set(stack_value, 3, 12, T.Cast("int64", 32))
            stack_tcode_1[3] = 0
            T.tvm_struct_set(stack_value, 4, 12, T.Cast("int64", 32))
            stack_tcode_1[4] = 0
            T.call_packed_lowered("sum_kernel", stack_value, stack_tcode, 0, 5)
        T.ret(0)
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: sequential
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(args: T.handle, arg_type_ids: T.handle("int32"), num_args: T.int32, out_ret_value: T.handle("void"), out_ret_tcode: T.handle("int32"), resource_handle: T.handle) -> T.int32:
        T.func_attr({"calling_conv": 1, "from_legacy_te_schedule": T.bool(True), "target": T.target({"keys": ["cpu"], "kind": "llvm", "tag": ""}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        stack_tcode: T.handle("int32") = T.tvm_stack_alloca("arg_tcode", 6)
        stack_tcode_1 = T.decl_buffer((T.uint64(6),), "int32", data=stack_tcode)
        stack_value: T.handle = T.tvm_stack_alloca("arg_value", 6)
        assert num_args == 2, "sum: num_args should be 2"
        arg_type_ids_1 = T.decl_buffer((2,), "int32", data=arg_type_ids)
        A_code: T.int32 = arg_type_ids_1[0]
        A_red_code: T.int32 = arg_type_ids_1[1]
        A: T.handle = T.tvm_struct_get(args, 0, 12, "handle")
        A_red: T.handle = T.tvm_struct_get(args, 1, 12, "handle")
        A_1: T.handle("float32") = T.tvm_struct_get(A, 0, 1, "handle")
        T.attr(A_1, "storage_alignment", 64)
        sum_A_shape: T.handle("int64") = T.tvm_struct_get(A, 0, 2, "handle")
        sum_A_shape_1 = T.decl_buffer((3,), "int64", data=sum_A_shape)
        sum_A_strides: T.handle("int64") = T.tvm_struct_get(A, 0, 3, "handle")
        sum_A_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_strides)
        dev_id: T.int32 = T.tvm_struct_get(A, 0, 9, "int32")
        A_red_1: T.handle("float32") = T.tvm_struct_get(A_red, 0, 1, "handle")
        T.attr(A_red_1, "storage_alignment", 64)
        sum_A_red_shape: T.handle("int64") = T.tvm_struct_get(A_red, 0, 2, "handle")
        sum_A_red_shape_1 = T.decl_buffer((1,), "int64", data=sum_A_red_shape)
        sum_A_red_strides: T.handle("int64") = T.tvm_struct_get(A_red, 0, 3, "handle")
        sum_A_red_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_red_strides)
        assert A_code == 3 or A_code == 13 or A_code == 7 or A_code == 4, "sum: Expect arg[0] to be pointer"
        assert A_red_code == 3 or A_red_code == 13 or A_red_code == 7 or A_red_code == 4, "sum: Expect arg[1] to be pointer"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert T.tvm_struct_get(A, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A, 0, 7, "uint16") == T.uint16(1), "sum.A.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_shape_1[0]) == 16, "Argument sum.A.shape[0] has an unsatisfied constraint: 16 == T.Cast(\"int32\", sum_A_shape[0])"
        assert T.Cast("int32", sum_A_shape_1[1]) == 4, "Argument sum.A.shape[1] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_shape[1])"
        assert T.Cast("int32", sum_A_shape_1[2]) == 3, "Argument sum.A.shape[2] has an unsatisfied constraint: 3 == T.Cast(\"int32\", sum_A_shape[2])"
        if not T.isnullptr(sum_A_strides):
            assert 1 == T.Cast("int32", sum_A_strides_1[2]) and 3 == T.Cast("int32", sum_A_strides_1[1]) and 12 == T.Cast("int32", sum_A_strides_1[0]), "sum.A.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A, 0, 8, "uint64"), "Argument sum.A.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A, 0, 10, "int32") == 2, "Argument sum.A.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A, 0, 10, \"int32\")"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert T.tvm_struct_get(A_red, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A_red, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A_red, 0, 7, "uint16") == T.uint16(1), "sum.A_red.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_red_shape_1[0]) == 4, "Argument sum.A_red.shape[0] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_red_shape[0])"
        if not T.isnullptr(sum_A_red_strides):
            assert 1 == T.Cast("int32", sum_A_red_strides_1[0]), "sum.A_red.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, "uint64"), "Argument sum.A_red.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A_red, 0, 10, "int32") == 2, "Argument sum.A_red.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A_red, 0, 10, \"int32\")"
        assert dev_id == T.tvm_struct_get(A_red, 0, 9, "int32"), "Argument sum.A_red.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(A_red, 0, 9, \"int32\")"
        A_2 = T.decl_buffer((16, 4, 3), data=A_1)
        A_red_2 = T.decl_buffer((4,), data=A_red_1)
        T.tvm_struct_set(stack_value, 0, 12, T.Cast("int64", 2))
        stack_tcode_1[0] = 0
        T.tvm_struct_set(stack_value, 1, 12, T.Cast("int64", dev_id))
        stack_tcode_1[1] = 0
        T.call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2)
        with T.attr(0, "compute_scope", "sum_compute_"):
            T.tvm_struct_set(stack_value, 0, 12, A_1)
            if T.isnullptr(A_1):
                stack_tcode_1[0] = 4
            else:
                stack_tcode_1[0] = 3
            T.tvm_struct_set(stack_value, 1, 12, A_red_1)
            if T.isnullptr(A_red_1):
                stack_tcode_1[1] = 4
            else:
                stack_tcode_1[1] = 3
            T.tvm_struct_set(stack_value, 2, 12, T.Cast("int64", 1))
            stack_tcode_1[2] = 0
            T.tvm_struct_set(stack_value, 3, 12, T.Cast("int64", 32))
            stack_tcode_1[3] = 0
            T.tvm_struct_set(stack_value, 4, 12, T.Cast("int64", 32))
            stack_tcode_1[4] = 0
            T.call_packed_lowered("sum_kernel", stack_value, stack_tcode, 0, 5)
        T.ret(0)
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: sequential
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.Filter
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.Filter
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"calling_conv": 2, "target": T.target({"arch": "sm_80", "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.kernel_launch_params": ["blockIdx.x", "threadIdx.y", "threadIdx.x"], "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tvm_warp_activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 16, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 8, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 4, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 2, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 1, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tvm_warp_shuffle(mask_1[0], red_buf0_1[0], 32 * threadIdx_y, 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.BindTarget
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.BindTarget
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"calling_conv": 2, "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.kernel_launch_params": ["blockIdx.x", "threadIdx.y", "threadIdx.x"], "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tvm_warp_activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 16, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 8, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 4, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 2, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 1, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tvm_warp_shuffle(mask_1[0], red_buf0_1[0], 32 * threadIdx_y, 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LowerWarpMemory
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LowerWarpMemory
[16:36:37] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"calling_conv": 2, "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.kernel_launch_params": ["blockIdx.x", "threadIdx.y", "threadIdx.x"], "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tvm_warp_activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 16, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 8, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 4, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 2, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 1, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tvm_warp_shuffle(mask_1[0], red_buf0_1[0], 32 * threadIdx_y, 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:38] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.Simplify
[16:36:38] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.Simplify
[16:36:38] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"calling_conv": 2, "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.kernel_launch_params": ["blockIdx.x", "threadIdx.y", "threadIdx.x"], "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tvm_warp_activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 16, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 8, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 4, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 2, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 1, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tvm_warp_shuffle(mask_1[0], red_buf0_1[0], threadIdx_y * 32, 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:38] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LowerCustomDatatypes
[16:36:38] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LowerCustomDatatypes
[16:36:38] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"calling_conv": 2, "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.kernel_launch_params": ["blockIdx.x", "threadIdx.y", "threadIdx.x"], "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tvm_warp_activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 16, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 8, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 4, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 2, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 1, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tvm_warp_shuffle(mask_1[0], red_buf0_1[0], threadIdx_y * 32, 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:38] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LowerDeviceStorageAccessInfo
[16:36:38] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LowerDeviceStorageAccessInfo
[16:36:38] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"calling_conv": 2, "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.kernel_launch_params": ["blockIdx.x", "threadIdx.y", "threadIdx.x"], "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tvm_warp_activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 16, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 8, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 4, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 2, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tvm_warp_shuffle_down(mask_1[0], red_buf0_1[0], 1, 32, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tvm_warp_shuffle(mask_1[0], red_buf0_1[0], threadIdx_y * 32, 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:38] /ssd2/liuchao52/tvm/src/ir/transform.cc:266: Warning: 6clc apply pass: tir.LowerIntrin
[16:36:38] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: tir.LowerIntrin
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__activemask
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_sync
[16:36:38] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"calling_conv": 2, "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.kernel_launch_params": ["blockIdx.x", "threadIdx.y", "threadIdx.x"], "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + T.shift_right(threadIdx_x, 4) < 3 and k0_k2_fused_outer * 2 + T.shift_right(threadIdx_x, 4) < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[T.Div(k0_k2_fused_outer * 32 + threadIdx_x, 3) * 12 + threadIdx_y * 3 + T.truncmod(k0_k2_fused_outer * 2 + threadIdx_x, 3)]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tir.cuda.__activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 16, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 8, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 4, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 2, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 1, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tir.cuda.__shfl_sync(mask_1[0], red_buf0_1[0], threadIdx_y * 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:38] /ssd2/liuchao52/tvm/src/ir/transform.cc:278: Warning: sequential
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__activemask
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_sync
[16:36:38] /ssd2/liuchao52/tvm/src/ir/transform.cc:279: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"calling_conv": 2, "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.kernel_launch_params": ["blockIdx.x", "threadIdx.y", "threadIdx.x"], "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + T.shift_right(threadIdx_x, 4) < 3 and k0_k2_fused_outer * 2 + T.shift_right(threadIdx_x, 4) < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[T.Div(k0_k2_fused_outer * 32 + threadIdx_x, 3) * 12 + threadIdx_y * 3 + T.truncmod(k0_k2_fused_outer * 2 + threadIdx_x, 3)]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tir.cuda.__activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 16, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 8, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 4, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 2, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 1, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tir.cuda.__shfl_sync(mask_1[0], red_buf0_1[0], threadIdx_y * 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__activemask
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_sync
[16:36:38] /ssd2/liuchao52/tvm/src/driver/driver_api.cc:423: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"calling_conv": 2, "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.kernel_launch_params": ["blockIdx.x", "threadIdx.y", "threadIdx.x"], "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + T.shift_right(threadIdx_x, 4) < 3 and k0_k2_fused_outer * 2 + T.shift_right(threadIdx_x, 4) < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[T.Div(k0_k2_fused_outer * 32 + threadIdx_x, 3) * 12 + threadIdx_y * 3 + T.truncmod(k0_k2_fused_outer * 2 + threadIdx_x, 3)]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tir.cuda.__activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 16, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 8, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 4, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 2, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 1, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tir.cuda.__shfl_sync(mask_1[0], red_buf0_1[0], threadIdx_y * 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:38] /ssd2/liuchao52/tvm/src/driver/driver_api.cc:500: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum(args: T.handle, arg_type_ids: T.handle("int32"), num_args: T.int32, out_ret_value: T.handle("void"), out_ret_tcode: T.handle("int32"), resource_handle: T.handle) -> T.int32:
        T.func_attr({"calling_conv": 1, "from_legacy_te_schedule": T.bool(True), "target": T.target({"keys": ["cpu"], "kind": "llvm", "tag": ""}), "tir.is_entry_func": T.bool(True), "tir.noalias": T.bool(True)})
        stack_tcode: T.handle("int32") = T.tvm_stack_alloca("arg_tcode", 6)
        stack_tcode_1 = T.decl_buffer((T.uint64(6),), "int32", data=stack_tcode)
        stack_value: T.handle = T.tvm_stack_alloca("arg_value", 6)
        assert num_args == 2, "sum: num_args should be 2"
        arg_type_ids_1 = T.decl_buffer((2,), "int32", data=arg_type_ids)
        A_code: T.int32 = arg_type_ids_1[0]
        A_red_code: T.int32 = arg_type_ids_1[1]
        A: T.handle = T.tvm_struct_get(args, 0, 12, "handle")
        A_red: T.handle = T.tvm_struct_get(args, 1, 12, "handle")
        A_1: T.handle("float32") = T.tvm_struct_get(A, 0, 1, "handle")
        T.attr(A_1, "storage_alignment", 64)
        sum_A_shape: T.handle("int64") = T.tvm_struct_get(A, 0, 2, "handle")
        sum_A_shape_1 = T.decl_buffer((3,), "int64", data=sum_A_shape)
        sum_A_strides: T.handle("int64") = T.tvm_struct_get(A, 0, 3, "handle")
        sum_A_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_strides)
        dev_id: T.int32 = T.tvm_struct_get(A, 0, 9, "int32")
        A_red_1: T.handle("float32") = T.tvm_struct_get(A_red, 0, 1, "handle")
        T.attr(A_red_1, "storage_alignment", 64)
        sum_A_red_shape: T.handle("int64") = T.tvm_struct_get(A_red, 0, 2, "handle")
        sum_A_red_shape_1 = T.decl_buffer((1,), "int64", data=sum_A_red_shape)
        sum_A_red_strides: T.handle("int64") = T.tvm_struct_get(A_red, 0, 3, "handle")
        sum_A_red_strides_1 = T.decl_buffer((0,), "int64", data=sum_A_red_strides)
        assert A_code == 3 or A_code == 13 or A_code == 7 or A_code == 4, "sum: Expect arg[0] to be pointer"
        assert A_red_code == 3 or A_red_code == 13 or A_red_code == 7 or A_red_code == 4, "sum: Expect arg[1] to be pointer"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert 3 == T.tvm_struct_get(A, 0, 4, "int32"), "sum.A.ndim is expected to equal 3"
        assert T.tvm_struct_get(A, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A, 0, 7, "uint16") == T.uint16(1), "sum.A.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_shape_1[0]) == 16, "Argument sum.A.shape[0] has an unsatisfied constraint: 16 == T.Cast(\"int32\", sum_A_shape[0])"
        assert T.Cast("int32", sum_A_shape_1[1]) == 4, "Argument sum.A.shape[1] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_shape[1])"
        assert T.Cast("int32", sum_A_shape_1[2]) == 3, "Argument sum.A.shape[2] has an unsatisfied constraint: 3 == T.Cast(\"int32\", sum_A_shape[2])"
        if not T.isnullptr(sum_A_strides):
            assert 1 == T.Cast("int32", sum_A_strides_1[2]) and 3 == T.Cast("int32", sum_A_strides_1[1]) and 12 == T.Cast("int32", sum_A_strides_1[0]), "sum.A.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A, 0, 8, "uint64"), "Argument sum.A.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A, 0, 10, "int32") == 2, "Argument sum.A.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A, 0, 10, \"int32\")"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert 1 == T.tvm_struct_get(A_red, 0, 4, "int32"), "sum.A_red.ndim is expected to equal 1"
        assert T.tvm_struct_get(A_red, 0, 5, "uint8") == T.uint8(2) and T.tvm_struct_get(A_red, 0, 6, "uint8") == T.uint8(32) and T.tvm_struct_get(A_red, 0, 7, "uint16") == T.uint16(1), "sum.A_red.dtype is expected to be float32"
        assert T.Cast("int32", sum_A_red_shape_1[0]) == 4, "Argument sum.A_red.shape[0] has an unsatisfied constraint: 4 == T.Cast(\"int32\", sum_A_red_shape[0])"
        if not T.isnullptr(sum_A_red_strides):
            assert 1 == T.Cast("int32", sum_A_red_strides_1[0]), "sum.A_red.strides: expected to be compact array"
            T.evaluate(0)
        assert T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, "uint64"), "Argument sum.A_red.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(A_red, 0, 8, \"uint64\")"
        assert T.tvm_struct_get(A_red, 0, 10, "int32") == 2, "Argument sum.A_red.device_type has an unsatisfied constraint: 2 == T.tvm_struct_get(A_red, 0, 10, \"int32\")"
        assert dev_id == T.tvm_struct_get(A_red, 0, 9, "int32"), "Argument sum.A_red.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(A_red, 0, 9, \"int32\")"
        A_2 = T.decl_buffer((16, 4, 3), data=A_1)
        A_red_2 = T.decl_buffer((4,), data=A_red_1)
        T.tvm_struct_set(stack_value, 0, 12, T.Cast("int64", 2))
        stack_tcode_1[0] = 0
        T.tvm_struct_set(stack_value, 1, 12, T.Cast("int64", dev_id))
        stack_tcode_1[1] = 0
        T.call_packed_lowered("__tvm_set_device", stack_value, stack_tcode, 0, 2)
        with T.attr(0, "compute_scope", "sum_compute_"):
            T.tvm_struct_set(stack_value, 0, 12, A_1)
            if T.isnullptr(A_1):
                stack_tcode_1[0] = 4
            else:
                stack_tcode_1[0] = 3
            T.tvm_struct_set(stack_value, 1, 12, A_red_1)
            if T.isnullptr(A_red_1):
                stack_tcode_1[1] = 4
            else:
                stack_tcode_1[1] = 3
            T.tvm_struct_set(stack_value, 2, 12, T.Cast("int64", 1))
            stack_tcode_1[2] = 0
            T.tvm_struct_set(stack_value, 3, 12, T.Cast("int64", 32))
            stack_tcode_1[3] = 0
            T.tvm_struct_set(stack_value, 4, 12, T.Cast("int64", 32))
            stack_tcode_1[4] = 0
            T.call_packed_lowered("sum_kernel", stack_value, stack_tcode, 0, 5)
        T.ret(0)
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__activemask
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_down_sync
[16:36:38] /ssd2/liuchao52/tvm/src/script/printer/tir/expr.cc:246: Warning: No TScriptPrinterName attribute for tir.cuda.__shfl_sync
[16:36:38] /ssd2/liuchao52/tvm/src/driver/driver_api.cc:501: Warning: # from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    I.module_attrs({"runtime": None})
    @T.prim_func
    def sum_kernel(A: T.handle("float32"), A_red: T.handle("float32")):
        T.func_attr({"calling_conv": 2, "target": T.target({"arch": "sm_80", "host": {"keys": ["cpu"], "kind": "llvm", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_global_func": T.bool(True), "tir.kernel_launch_params": ["blockIdx.x", "threadIdx.y", "threadIdx.x"], "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        red_buf0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + T.shift_right(threadIdx_x, 4) < 3 and k0_k2_fused_outer * 2 + T.shift_right(threadIdx_x, 4) < 3:
                    A_1 = T.Buffer((192,), data=A)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[T.Div(k0_k2_fused_outer * 32 + threadIdx_x, 3) * 12 + threadIdx_y * 3 + T.truncmod(k0_k2_fused_outer * 2 + threadIdx_x, 3)]
        red_buf0_1 = T.Buffer((1,), data=red_buf0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            mask = T.allocate([1], "uint32", "local")
            t0 = T.allocate([1], "float32", "local")
            red_buf0_1[0] = A_red_rf_1[0]
            mask_1 = T.Buffer((1,), "uint32", data=mask, scope="local")
            mask_1[0] = T.tir.cuda.__activemask()
            t0_1 = T.Buffer((1,), data=t0, scope="local")
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 16, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 8, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 4, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 2, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            t0_1[0] = T.tir.cuda.__shfl_down_sync(mask_1[0], red_buf0_1[0], 1, 32)
            red_buf0_1[0] = red_buf0_1[0] + t0_1[0]
            red_buf0_1[0] = T.tir.cuda.__shfl_sync(mask_1[0], red_buf0_1[0], threadIdx_y * 32, 32)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red_1 = T.Buffer((4,), data=A_red)
            A_red_1[threadIdx_y] = red_buf0_1[0]
[16:36:38] /ssd2/liuchao52/tvm/src/target/opt/build_cuda_on.cc:144: Warning: 
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)
#define __shfl_sync(mask, var, lane, width) \
        __shfl((var), (lane), (width))

#define __shfl_down_sync(mask, var, offset, width) \
        __shfl_down((var), (offset), (width))

#define __shfl_up_sync(mask, var, offset, width) \
        __shfl_up((var), (offset), (width))
#endif


#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \
     (__CUDACC_VER_MAJOR__ > 11))
#define TVM_ENABLE_L2_PREFETCH 1
#else
#define TVM_ENABLE_L2_PREFETCH 0
#endif

#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) sum_kernel(float* __restrict__ A, float* __restrict__ A_red) {
  float A_red_rf[1];
  float red_buf0[1];
  A_red_rf[0] = 0.000000e+00f;
  for (int k0_k2_fused_outer = 0; k0_k2_fused_outer < 2; ++k0_k2_fused_outer) {
    if (((int)threadIdx.y) < 4) {
      if ((((k0_k2_fused_outer * 2) + (((int)threadIdx.x) >> 4)) < 3) && (((k0_k2_fused_outer * 2) + (((int)threadIdx.x) >> 4)) < 3)) {
        A_red_rf[0] = (A_red_rf[0] + A[((((((k0_k2_fused_outer * 32) + ((int)threadIdx.x)) / 3) * 12) + (((int)threadIdx.y) * 3)) + (((k0_k2_fused_outer * 2) + ((int)threadIdx.x)) % 3))]);
      }
    }
  }
  uint mask[1];
  float t0[1];
  red_buf0[0] = A_red_rf[0];
  mask[0] = __activemask();
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 32), 32);
  if ((((int)threadIdx.x) == 0) && (((int)threadIdx.y) < 4)) {
    A_red[((int)threadIdx.y)] = red_buf0[0];
  }
}


# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def sum(A: T.Buffer((16, 4, 3), "float32"), A_red: T.Buffer((4,), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        blockIdx_x = T.launch_thread("blockIdx.x", 1)
        A_red_rf = T.allocate([1], "float32", "local")
        reduce_temp0 = T.allocate([1], "float32", "local")
        threadIdx_y = T.launch_thread("threadIdx.y", 32)
        threadIdx_x = T.launch_thread("threadIdx.x", 32)
        A_red_rf_1 = T.Buffer((1,), data=A_red_rf, scope="local", align=4)
        A_red_rf_1[0] = T.float32(0)
        for k0_k2_fused_outer in range(2):
            if threadIdx_y < 4:
                if k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3 and k0_k2_fused_outer * 2 + threadIdx_x // 16 < 3:
                    A_1 = T.Buffer((192,), data=A.data)
                    A_red_rf_1[0] = A_red_rf_1[0] + A_1[(k0_k2_fused_outer * 32 + threadIdx_x) // 3 * 12 + threadIdx_y * 3 + (k0_k2_fused_outer * 2 + threadIdx_x) % 3]
        reduce_temp0_1 = T.Buffer((1,), data=reduce_temp0, scope="local")
        with T.attr(T.comm_reducer(lambda x, y: x + y, [T.float32(0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), A_red_rf_1[0], T.bool(True), reduce_temp0_1[0], threadIdx_x)
        if threadIdx_x == 0 and threadIdx_y < 4:
            A_red[threadIdx_y] = reduce_temp0_1[0] xxx yy zz
